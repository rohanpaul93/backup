## standard model parameters
x = x.indep,
y = y.dep,
training_frame = train,
validation_frame = valid,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 10000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## early stopping based on timeout (no model should take more than 1 hour - modify as needed)
max_runtime_secs = 3600,
## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "AUC",
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 1234
)
sortedGrid <- h2o.getGrid("final_grid", sort_by = "auc", decreasing = TRUE)
sortedGrid
gbm.model <- h2o.getModel(sortedGrid@model_ids[[1]])
gbm.model@parameters
gbm_final_model <- h2o.gbm(y = y.dep, x = x.indep, training_frame = train.h2o,
ntrees = 10000, learn_rate = 0.05, learn_rate_annealing = 0.99,
max_depth = 7, distribution = "bernoulli", sample_rate = 0.57,
col_sample_rate = 0.92, col_sample_rate_change_per_level = 1.04,
min_split_improvement = 0, histogram_type = "QuantilesGlobal",
score_tree_interval = 10, nbins = 256, nbins_cats = 16, stopping_rounds = 5,
stopping_metric = "AUC", stopping_tolerance = 0.0001,
nfolds = 5, seed = 1234)
gbm_final_model@model$cross_validation_metrics
setwd("C:/Users/rohan.thekanath/Desktop/personal work/practice/wns")
library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(ggthemes)
library(caret)
library(gridExtra)
library(corrplot)
library(h2o)
library(caTools)
library(xgboost)
list.files()
train_data <- read.csv("train_LZdllcl.csv", stringsAsFactors = F)
test_data <- read.csv("test_2umaH9m.csv", stringsAsFactors = F)
dim(train_data) # 54808 rows with 14 cols
dim(test_data) # 23490 rows with 13 cols
str(train_data)
train_data$employee_id <- as.factor(train_data$employee_id)
test_data$employee_id <- as.factor(test_data$employee_id)
train_data$is_promoted <- as.factor(ifelse(train_data$is_promoted == 0, "No", "Yes"))
sapply(train_data, function(x) sum(is.na(x))) # 4124 missing values for previous rating
sapply(test_data, function(x) sum(is.na(x))) # 1812 missing values for previous rating
sapply(train_data, function(x) sum(x == "")) # 2409 Blank values in education
sapply(test_data, function(x) sum(x == "")) # 1034 Blank values in education
character_variables <- as.integer(which(sapply(train_data, function(x) is.character(x))))
train_data_factor <- as.data.frame(sapply(train_data[,character_variables], function(x) as.factor(x)))
train_data[,character_variables] <-  train_data_factor
character_variables_test <- as.integer(which(sapply(test_data, function(x) is.character(x))))
test_data_factor <- as.data.frame(sapply(test_data[,character_variables], function(x) as.factor(x)))
test_data[,character_variables_test] <-  test_data_factor
rm(train_data_factor, test_data_factor)
summary(train_data) # At a glance there aren't any unusual values
summary(test_data)
prop.table(table(train_data$is_promoted))
unique(train_data$length_of_service[is.na(train_data$previous_year_rating)]) # 1.
train_data$previous_year_rating <- paste("Rating", train_data$previous_year_rating, sep = "_")
train_data$previous_year_rating <- as.factor(train_data$previous_year_rating)
test_data$previous_year_rating <- paste("Rating", test_data$previous_year_rating, sep = "_")
test_data$previous_year_rating <- as.factor(test_data$previous_year_rating)
train_data$education <- as.character(train_data$education)
test_data$education <- as.character(test_data$education)
train_data$education[train_data$education == "" | train_data$education == " "] = "Unknown"
test_data$education[test_data$education == "" | test_data$education == " "] = "Unknown"
train_data$education <- as.factor(train_data$education)
test_data$education <- as.factor(test_data$education)
Plotter_Categorical <- function(data, source_var, target_var){
p1 <- ggplot(data, aes(x = data[,c(source_var)], fill = data[,c(target_var)])) + geom_bar() +
scale_fill_tableau() + theme_solarized() + theme(axis.text.x = element_text(angle = 90)) +
geom_text(stat = "count", aes(label = ..count..), vjust = -0.1, position = "nudge") +
labs(x = source_var, y = target_var) + theme(legend.title = element_blank())
p2 <- ggplot(data, aes(x = data[,c(source_var)], fill = data[,c(target_var)])) + geom_bar(position = "fill") +
scale_fill_tableau() + theme_solarized() + theme(axis.text.x = element_text(angle = 90)) +
labs(x = source_var, y = target_var) + theme(legend.title = element_blank())
x11()
grid.arrange(p1, p2)
}
Plotter_Categorical(train_data, "department", "is_promoted")
Plotter_Categorical(train_data, "region", "is_promoted")
round(prop.table(table(train_data$region, train_data$is_promoted),1),2)
train_data <- train_data %>% dplyr::select(-region)
test_data <- test_data %>% dplyr::select(-region)
train_data$KPIs_met..80. <- ifelse(train_data$KPIs_met..80. == 0, "No", "Yes")
test_data$KPIs_met..80. <- ifelse(test_data$KPIs_met..80. == 0, "No", "Yes")
train_data$awards_won. = ifelse(train_data$awards_won. == 1, "Awards_won", "No_awards")
test_data$awards_won. = ifelse(test_data$awards_won. == 1, "Awards_won", "No_awards")
cor_matrix <- cor(train_data[,c(6,7,9,12)])
str(train_data)
corrplot(cor_matrix, method = "number", type = "upper", bg = "lightgreen")
train_data <- train_data %>% select(-length_of_service)
test_data <- test_data %>% select(-length_of_service)
train_data$KPIs_met..80. = as.factor(train_data$KPIs_met..80.)
train_data$awards_won. = as.factor(train_data$awards_won.)
test_data$KPIs_met..80. = as.factor(test_data$KPIs_met..80.)
test_data$awards_won. = as.factor(test_data$awards_won.)
is_promoted <- train_data$is_promoted
combined_data <- rbind(train_data[,-ncol(train_data)], test_data)
combined_data_with_dummies <- combined_data
dummy <- as.data.frame(model.matrix(~department, data = combined_data_with_dummies))
combined_data_with_dummies <- cbind(combined_data_with_dummies[,-2], dummy[,-1])
dummy <- as.data.frame(model.matrix(~education, data = combined_data_with_dummies))
combined_data_with_dummies <- cbind(combined_data_with_dummies[,-2], dummy[,-1])
combined_data_with_dummies$gender <- ifelse(combined_data_with_dummies$gender == "m",
1, 0)
combined_data_with_dummies$gender <- as.factor(combined_data_with_dummies$gender)
dummy <- as.data.frame(model.matrix(~recruitment_channel, data = combined_data_with_dummies))
combined_data_with_dummies <- cbind(combined_data_with_dummies[,-3], dummy[,-1])
dummy <- as.data.frame(model.matrix(~previous_year_rating, data = combined_data_with_dummies))
combined_data_with_dummies <- cbind(combined_data_with_dummies[,-5], dummy[,-1])
combined_data_with_dummies$KPIs_met..80. <- ifelse(combined_data_with_dummies$KPIs_met..80. == "Yes",1,0)
combined_data_with_dummies$awards_won. <- ifelse(combined_data_with_dummies$awards_won. == "Awards_won",1,0)
combined_data_with_dummies$KPIs_met..80. <- as.factor(combined_data_with_dummies$KPIs_met..80.)
combined_data_with_dummies$awards_won. <- as.factor(combined_data_with_dummies$awards_won.)
sapply(combined_data_with_dummies, function(x) sum(is.na(x))) # No missing values
train_data_with_dummies <- cbind(combined_data_with_dummies[1:nrow(train_data),], is_promoted)
test_data_with_dummies <- combined_data_with_dummies[(nrow(train_data) + 1):nrow(combined_data_with_dummies),]
train_data_with_dummies$is_promoted <- ifelse(train_data_with_dummies$is_promoted == "Yes",1,0)
set.seed(123)
indices = sample.split(train_data_with_dummies$is_promoted, SplitRatio = 0.75)
train_data_with_dummies_2 = train_data_with_dummies[indices,]
validation_data_with_dummies = train_data_with_dummies[!(indices),]
gbm.model <- h2o.gbm(y = y.dep, x = x.indep, training_frame = train.h2o,
ntrees = 1000, learn_rate = 0.1, seed = 123)
train_data_with_dummies.h2o <- as.h2o(train_data_with_dummies_2)
validation_data_with_dummies.h2o <- as.h2o(validation_data_with_dummies)
test_data_with_dummies.h2o <- as.h2o(test_data_with_dummies)
y.dep <- 26
x.indep <- 2:25
h2o.init(nthreads = -1)
train_data_with_dummies.h2o <- as.h2o(train_data_with_dummies_2)
validation_data_with_dummies.h2o <- as.h2o(validation_data_with_dummies)
test_data_with_dummies.h2o <- as.h2o(test_data_with_dummies)
y.dep <- 26
x.indep <- 2:25
gbm.model <- h2o.gbm(y = y.dep, x = x.indep, training_frame = train.h2o,
ntrees = 1000, learn_rate = 0.1, seed = 123)
train.h2o <- as.h2o(train_data)
test.h2o <- as.h2o(test_data)
y.dep = 12
x.indep = 2:11
gbm.model <- h2o.gbm(y = y.dep, x = x.indep, training_frame = train.h2o,
ntrees = 1000, learn_rate = 0.1, seed = 123)
summary(gbm.model)
predictions_gbm <- as.data.frame(h2o.predict(gbm.model, test.h2o))
splits <- h2o.splitFrame(
data = train.h2o,
ratios = c(0.6,0.2),   ## only need to specify 2 fractions, the 3rd is implied
destination_frames = c("train.hex", "valid.hex", "test.hex"), seed = 1234
)
train <- splits[[1]]
valid <- splits[[2]]
test  <- splits[[3]]
hyper_params = list( max_depth = seq(1,29,2) )
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## full Cartesian hyper-parameter search
search_criteria = list(strategy = "Cartesian"),
## which algorithm to run
algorithm="gbm",
## identifier for the grid, to later retrieve it
grid_id="depth_grid",
## standard model parameters
x = x.indep,
y = y.dep,
training_frame = train,
validation_frame = valid,
## more trees is better if the learning rate is small enough
## here, use "more than enough" trees - we have early stopping
ntrees = 10000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## fix a random number generator seed for reproducibility
seed = 1234,
## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
stopping_rounds = 5,
stopping_tolerance = 1e-4,
stopping_metric = "AUC",
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10
)
sortedGrid <- h2o.getGrid("depth_grid", sort_by="auc", decreasing = TRUE)
sortedGrid
minDepth = 2
maxDepth = 10
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(minDepth,maxDepth,1),
## search a large space of row sampling rates per tree
sample_rate = seq(0.2,1,0.01),
## search a large space of column sampling rates per split
col_sample_rate = seq(0.2,1,0.01),
## search a large space of column sampling rates per tree
col_sample_rate_per_tree = seq(0.2,1,0.01),
## search a large space of how column sampling per split should change as a function of the depth of the split
col_sample_rate_change_per_level = seq(0.9,1.1,0.01),
## search a large space of the number of min rows in a terminal node
min_rows = 2^seq(0,log2(nrow(train.h2o))-1,1),
## search a large space of the number of bins for split-finding for continuous and integer columns
nbins = 2^seq(4,10,1),
## search a large space of the number of bins for split-finding for categorical columns
nbins_cats = 2^seq(4,12,1),
## search a few minimum required relative error improvement thresholds for a split to happen
min_split_improvement = c(0,1e-8,1e-6,1e-4),
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
histogram_type = c("UniformAdaptive","QuantilesGlobal","RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete",
## limit the runtime to 60 minutes
max_runtime_secs = 3600,
## build no more than 100 models
max_models = 100,
## random number generator seed to make sampling of parameter combinations reproducible
seed = 1234,
## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
stopping_rounds = 5,
stopping_metric = "AUC",
stopping_tolerance = 1e-3
)
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## hyper-parameter search configuration (see above)
search_criteria = search_criteria,
## which algorithm to run
algorithm = "gbm",
## identifier for the grid, to later retrieve it
grid_id = "final_grid",
## standard model parameters
x = x.indep,
y = y.dep,
training_frame = train,
validation_frame = valid,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 10000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## early stopping based on timeout (no model should take more than 1 hour - modify as needed)
max_runtime_secs = 3600,
## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "AUC",
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 1234
)
sortedGrid <- h2o.getGrid("final_grid", sort_by = "auc", decreasing = TRUE)
sortedGrid
library(glmnet)
install.packages('glmnet')
age     <- c(4, 8, 7, 12, 6, 9, 10, 14, 7)
gender  <- as.factor(c(1, 0, 1, 1, 1, 0, 1, 0, 0))
bmi_p   <- c(0.86, 0.45, 0.99, 0.84, 0.85, 0.67, 0.91, 0.29, 0.88)
m_edu   <- as.factor(c(0, 1, 1, 2, 2, 3, 2, 0, 1))
p_edu   <- as.factor(c(0, 2, 2, 2, 2, 3, 2, 0, 0))
f_color <- as.factor(c("blue", "blue", "yellow", "red", "red", "yellow",
"yellow", "red", "yellow"))
asthma <- c(1, 1, 0, 1, 0, 0, 0, 1, 1)
xfactors <- model.matrix(asthma ~ gender + m_edu + p_edu + f_color)[, -1]
x        <- as.matrix(data.frame(age, bmi_p, xfactors))
View(x)
View(x)
View(x)
glmmod <- glmnet(x, y=as.factor(asthma), alpha=1, family="binomial")
library(glmnet)
glmmod <- glmnet(x, y=as.factor(asthma), alpha=1, family="binomial")
colnames(x)
str(asthama)
str(asthma)
as.factor(asthma)
plot(glmmod, xvar="lambda")
coef(glmmod)[, 10]
setwd("C:/Users/rohan.thekanath/Desktop/personal work/practice/wns")
rm(list = ls())
library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(ggthemes)
library(caret)
library(gridExtra)
library(corrplot)
library(h2o)
library(caTools)
library(xgboost)
train_data <- read.csv("train_LZdllcl.csv", stringsAsFactors = F)
test_data <- read.csv("test_2umaH9m.csv", stringsAsFactors = F)
train_data$employee_id <- as.factor(train_data$employee_id)
test_data$employee_id <- as.factor(test_data$employee_id)
train_data$is_promoted <- as.factor(ifelse(train_data$is_promoted == 0, "No", "Yes"))
character_variables <- as.integer(which(sapply(train_data, function(x) is.character(x))))
train_data_factor <- as.data.frame(sapply(train_data[,character_variables], function(x) as.factor(x)))
train_data[,character_variables] <-  train_data_factor
character_variables_test <- as.integer(which(sapply(test_data, function(x) is.character(x))))
test_data_factor <- as.data.frame(sapply(test_data[,character_variables], function(x) as.factor(x)))
test_data[,character_variables_test] <-  test_data_factor
rm(train_data_factor, test_data_factor)
train_data$previous_year_rating <- paste("Rating", train_data$previous_year_rating, sep = "_")
train_data$previous_year_rating <- as.factor(train_data$previous_year_rating)
test_data$previous_year_rating <- paste("Rating", test_data$previous_year_rating, sep = "_")
test_data$previous_year_rating <- as.factor(test_data$previous_year_rating)
train_data$education <- as.character(train_data$education)
test_data$education <- as.character(test_data$education)
train_data$education[train_data$education == "" | train_data$education == " "] = "Unknown"
test_data$education[test_data$education == "" | test_data$education == " "] = "Unknown"
train_data$education <- as.factor(train_data$education)
test_data$education <- as.factor(test_data$education)
train_data <- train_data %>% dplyr::select(-region)
test_data <- test_data %>% dplyr::select(-region)
train_data$KPIs_met..80. <- ifelse(train_data$KPIs_met..80. == 0, "No", "Yes")
test_data$KPIs_met..80. <- ifelse(test_data$KPIs_met..80. == 0, "No", "Yes")
Plotter_Categorical(train_data, "KPIs_met..80.", "is_promoted")
train_data$awards_won. = ifelse(train_data$awards_won. == 1, "Awards_won", "No_awards")
test_data$awards_won. = ifelse(test_data$awards_won. == 1, "Awards_won", "No_awards")
cor_matrix <- cor(train_data[,c(6,7,9,12)])
train_data <- train_data %>% select(-length_of_service)
test_data <- test_data %>% select(-length_of_service)
train_data$KPIs_met..80. = as.factor(train_data$KPIs_met..80.)
train_data$awards_won. = as.factor(train_data$awards_won.)
test_data$KPIs_met..80. = as.factor(test_data$KPIs_met..80.)
test_data$awards_won. = as.factor(test_data$awards_won.)
is_promoted <- train_data$is_promoted
combined_data <- rbind(train_data[,-ncol(train_data)], test_data)
combined_data_with_dummies <- combined_data
dummy <- as.data.frame(model.matrix(~department, data = combined_data_with_dummies))
combined_data_with_dummies <- cbind(combined_data_with_dummies[,-2], dummy[,-1])
dummy <- as.data.frame(model.matrix(~education, data = combined_data_with_dummies))
combined_data_with_dummies <- cbind(combined_data_with_dummies[,-2], dummy[,-1])
combined_data_with_dummies$gender <- ifelse(combined_data_with_dummies$gender == "m",
1, 0)
combined_data_with_dummies$gender <- as.factor(combined_data_with_dummies$gender)
dummy <- as.data.frame(model.matrix(~recruitment_channel, data = combined_data_with_dummies))
combined_data_with_dummies <- cbind(combined_data_with_dummies[,-3], dummy[,-1])
dummy <- as.data.frame(model.matrix(~previous_year_rating, data = combined_data_with_dummies))
combined_data_with_dummies <- cbind(combined_data_with_dummies[,-5], dummy[,-1])
combined_data_with_dummies$KPIs_met..80. <- ifelse(combined_data_with_dummies$KPIs_met..80. == "Yes",1,0)
combined_data_with_dummies$awards_won. <- ifelse(combined_data_with_dummies$awards_won. == "Awards_won",1,0)
combined_data_with_dummies$KPIs_met..80. <- as.factor(combined_data_with_dummies$KPIs_met..80.)
combined_data_with_dummies$awards_won. <- as.factor(combined_data_with_dummies$awards_won.)
train_data_with_dummies <- cbind(combined_data_with_dummies[1:nrow(train_data),], is_promoted)
test_data_with_dummies <- combined_data_with_dummies[(nrow(train_data) + 1):nrow(combined_data_with_dummies),]
train_data_with_dummies$is_promoted <- ifelse(train_data_with_dummies$is_promoted == "Yes",1,0)
set.seed(123)
indices = sample.split(train_data_with_dummies$is_promoted, SplitRatio = 0.75)
train_data_with_dummies_2 = train_data_with_dummies[indices,]
validation_data_with_dummies = train_data_with_dummies[!(indices),]
h2o.init(nthreads = -1)
train_data_with_dummies.h2o <- as.h2o(train_data_with_dummies_2)
validation_data_with_dummies.h2o <- as.h2o(validation_data_with_dummies)
test_data_with_dummies.h2o <- as.h2o(test_data_with_dummies)
y.dep <- 26
x.indep <- 2:25
gbm.model <- h2o.gbm(y = y.dep, x = x.indep, training_frame = train.h2o,
ntrees = 1000, learn_rate = 0.1, seed = 123)
train.h2o <- as.h2o(train_data)
test.h2o <- as.h2o(test_data)
y.dep = 12
x.indep = 2:11
gbm.model <- h2o.gbm(y = y.dep, x = x.indep, training_frame = train.h2o,
ntrees = 1000, learn_rate = 0.1, seed = 123)
summary(gbm.model)
splits <- h2o.splitFrame(
data = train.h2o,
ratios = c(0.6,0.2),   ## only need to specify 2 fractions, the 3rd is implied
destination_frames = c("train.hex", "valid.hex", "test.hex"), seed = 1234
)
train <- splits[[1]]
valid <- splits[[2]]
test  <- splits[[3]]
hyper_params = list( max_depth = seq(1,29,2) )
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## full Cartesian hyper-parameter search
search_criteria = list(strategy = "Cartesian"),
## which algorithm to run
algorithm="gbm",
## identifier for the grid, to later retrieve it
grid_id="depth_grid",
## standard model parameters
x = x.indep,
y = y.dep,
training_frame = train,
validation_frame = valid,
## more trees is better if the learning rate is small enough
## here, use "more than enough" trees - we have early stopping
ntrees = 10000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## fix a random number generator seed for reproducibility
seed = 1234,
## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
stopping_rounds = 5,
stopping_tolerance = 1e-4,
stopping_metric = "AUC",
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10
)
sortedGrid <- h2o.getGrid("depth_grid", sort_by="auc", decreasing = TRUE)
sortedGrid
minDepth = 2
maxDepth = 10
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(minDepth,maxDepth,1),
## search a large space of row sampling rates per tree
sample_rate = seq(0.2,1,0.01),
## search a large space of column sampling rates per split
col_sample_rate = seq(0.2,1,0.01),
## search a large space of column sampling rates per tree
col_sample_rate_per_tree = seq(0.2,1,0.01),
## search a large space of how column sampling per split should change as a function of the depth of the split
col_sample_rate_change_per_level = seq(0.9,1.1,0.01),
## search a large space of the number of min rows in a terminal node
min_rows = 2^seq(0,log2(nrow(train.h2o))-1,1),
## search a large space of the number of bins for split-finding for continuous and integer columns
nbins = 2^seq(4,10,1),
## search a large space of the number of bins for split-finding for categorical columns
nbins_cats = 2^seq(4,12,1),
## search a few minimum required relative error improvement thresholds for a split to happen
min_split_improvement = c(0,1e-8,1e-6,1e-4),
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
histogram_type = c("UniformAdaptive","QuantilesGlobal","RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete",
## limit the runtime to 60 minutes
max_runtime_secs = 3600,
## build no more than 100 models
max_models = 100,
## random number generator seed to make sampling of parameter combinations reproducible
seed = 1234,
## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
stopping_rounds = 5,
stopping_metric = "AUC",
stopping_tolerance = 1e-3
)
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## hyper-parameter search configuration (see above)
search_criteria = search_criteria,
## which algorithm to run
algorithm = "gbm",
## identifier for the grid, to later retrieve it
grid_id = "final_grid",
## standard model parameters
x = x.indep,
y = y.dep,
training_frame = train,
validation_frame = valid,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 10000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## early stopping based on timeout (no model should take more than 1 hour - modify as needed)
max_runtime_secs = 3600,
## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "AUC",
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 1234
)
sortedGrid
gbm.model <- h2o.getModel(sortedGrid@model_ids[[1]])
gbm.model@parameters
